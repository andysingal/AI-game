* [GitHub Copilot](https://github.com/features/copilot): assistente de c√≥digo desenvolvido pelo GitHub baseado em modelos da OpenAI `[assistant co-pilots]` `[page]`
* [Cursor](https://www.cursor.so/): Chat and edit in an editor built for pair-programming with AI `[assistant co-pilots]` `[page]`
* [Metabob](https://metabob.com/): Generative¬†AI for refactoring and debugging code `[assistant co-pilots]` `[page]`
* [Bard for Development](https://twitter.com/Prathkum/status/1656736988651995136?s=20):  `[assistant co-pilots]` `[tweet]`
* [The Ethics of GitHub Copilot](https://codinginterviewsmadesimple.substack.com/p/the-ethics-of-github-copilot-storytime): Are Code Generating Models like Copilot and ChatGPT ethical or useful? Viewed from the lens of a software developer. `[assistant co-pilots]` `[post]`
* [Created Developer Assitant](https://twitter.com/AiShivam/status/1658114538921762821?s=20): Created Developer Assitant is a platform designed to assist developers in efficiently analyzing, understanding, and fixing code within GitHub repositories. `[assistant co-pilots]` `[tweet]`
* [GPTutor](https://arxiv.org/abs/2305.01863): Create with Chat-GPT to assist in explaining the code `[assistant co-pilots]` `[paper]`
* [The Impact of AI on Developer Productivity: Evidence from GitHub Copilot](https://arxiv.org/abs/2302.06590):  `[assistant co-pilots]` `[]`
* [Bhttps://www.freecodecamp.org/news/build-a-full-stack-application-using-chatgpt/GPT](https://www.freecodecamp.org/news/build-a-full-stack-application-using-chatgpt/):  `[assistant co-pilots, outros, prompt engineering]` `[video]`
* [Alpaca: A Strong, Replicable Instruction-Following Model](https://crfm.stanford.edu/2023/03/13/alpaca.html): We introduce¬†Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. `[fine-tuning]` `[paper]`
* [LLaVa Lightining](https://www.linkedin.com/feed/update/urn:li:activity:7060782755025993728?utm_source=share&utm_medium=member_desktop): a new way to train a lightweight, multimodal GPT-4 model in just three hours for only $40. `[fine-tuning]` `[post]`
* [FINETUNED LANGUAGE MODELS ARE ZERO-SHOT LEARNERS](https://arxiv.org/pdf/2109.01652.pdf): This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning‚Äîfinetuning language models on a collection of datasets described via instructions‚Äîsubstantially improves zeroshot performance on unseen tasks. `[fine-tuning]` `[paper]`
* [Learn How To Finetune The Redpajama LLM](https://twitter.com/johnrobinsn/status/1658155513270370305?s=20): An opensource Llama-clone suitable for commercial dev. `[fine-tuning]` `[tweet]`
* [QLoRA 4-bit finetuning in a single GPU](https://twitter.com/Tim_Dettmers/status/1661379354507476994):  `[fine-tuning]` `[tweet]`
* [Improving LLMs with chain of thought](https://twitter.com/cwolferesearch/status/1650988783897133059): we can drastically improve LLMs reasoning performance using simple techniques that require no fine-tuning or task-specific verifiers `[llms plugins & tools]` `[tweet]`
* [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629): In this paper, we explore the use of LLMs to generate both reasoning traces and task-specific actions in an interleaved manner `[llms plugins & tools]` `[paper]`
* [Transformers Agents](https://huggingface.co/docs/transformers/transformers_agents): provides a natural language API on top of Hugging Face's transformers library `[llms plugins & tools]` `[page]`
* [FlowiseAI](https://flowiseai.com/): Open source UI visual tool to build your customized LLM flow using LangchainJS, written in Node Typescript/Javascript¬† Star ‚≠ê Github `[llms plugins & tools]` `[page]`
* [e2b - Your own virtual developer](https://github.com/e2b-dev/e2b): Your own virtual developer `[llms plugins & tools]` `[github]`
* [flowiseai - Build LLMs Apps Easily](https://flowiseai.com/):  `[llms plugins & tools]` `[]`
* [MiniChain](https://srush.github.io/MiniChain/): Aims to implement the core prompt chaining functionality in a tiny digestable library `[llms plugins & tools, prompt engineering]` `[github]`
* [Kor](https://eyurtsev.github.io/kor/): This is a half-baked prototype that ‚Äúhelps‚Äù you extract structured data from text using large language models (LLMs) `[llms plugins & tools, prompt engineering]` `[github]`
* [Promptify](https://promptify.readthedocs.io/): Solve NLP Problems with LLM's & Easily generate different NLP Task prompts for popular generative models like GPT, PaLM, and more with Promptify `[llms plugins & tools, prompt engineering]` `[github]`
* [Pandas AI](https://github.com/gventuri/pandas-ai): Python library that integrates generative artificial intelligence capabilities into Pandas, making dataframes conversational `[outros]` `[github]`
* [Goodbye ChatGPT: These (New) AI Tools Will Leave You Speechless](https://medium.com/swlh/goodbye-chatgpt-these-new-ai-tools-will-leave-you-speechless-1a923bb143ec):  `[outros]` `[post]`
* [New Programming Abstraction](https://twitter.com/DrJimFan/status/1657782710344249344?s=20): "- Specify the structured schema that your app desires. - Wrap it in an LLM decorator to parse unstructured user input. Prompting fades into the background. LLM is now in ""ambient mode‚Äù. - Use the object seamlessly in the rest of Software 1.0 stack. " `[outros]` `[tweet]`
* [AutoPR](https://github.com/irgolic/AutoPR): Fix issues with AI-generated pull requests, powered by ChatGPT `[outros]` `[github]`
* [Introducing `smol-developer`](https://twitter.com/swyx/status/1657892220492738560?s=20): ‚ñ∏ Human-centric, coherent whole program synthesis ‚ñ∏ your own junior developer ‚ñ∏ develop, debug, decompile ‚ñ∏ open source: https://t.co/HNveJvgo7M ‚ñ∏ 200 LOC, half english `[outros]` `[tweet]`
* [Context-Free Grammar Parsing with LLMs](https://matt-rickard.com/context-free-grammar-parsing-with-llms): Last week, I open-sourced a method to coerce LLMs into only generating a specific structure for a given regex pattern (https://matt-rickard.com/rellm¬†on Github). The library has proven extremely useful for a handful of tasks I‚Äôve been doing with LLMs (everything from categorization to simple agent automation). `[outros]` `[post]`
* [Teaching Large Language Models to Self-Debug](https://arxiv.org/abs/2304.05128): In this work, we propose Self-Debugging, which teaches a large language model to debug its predicted program via few-shot demonstrations. `[outros]` `[paper]`
* [k8sgpt](https://k8sgpt.ai/): +tool for scanning your kubernetes clusters, diagnosing and triaging issues in simple english. `[outros]` `[page]`
* [A guide to prompting AI (for what it is worth)](https://www.oneusefulthing.org/p/a-guide-to-prompting-ai-for-what): A brief guide to prompting `[prompt engineering]` `[post]`
* [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/abs/2201.11903): chain of thought (a series of intermediate reasoning steps) significantly improves the ability of large language models to perform complex reasoning. `[prompt engineering]` `[paper]`
* [Masterclass: AI-driven Development for Programmers](https://youtu.be/iO1mwxPNP5A): By Fireship. Learn how build a React app with Typescript using with AI prompting and other tricks to speed up development. `[prompt engineering]` `[video]`
* [Twitter Algo Explainer -- by Matt Shumer](https://twitter.com/mattshumer_/status/1654173953387003917?s=20): I used GPT-4-32K (+ other models) to analyze hundreds of files and explain how https://twitter.com/Twitter's open-source algorithm works `[prompt engineering]` `[tweet]`
* [Automatic Prompt Engineering Techniques](https://twitter.com/cwolferesearch/status/1654251652956712961?s=20): automated prompt engineering via continuous updates (e.g., via SGD) to a prompt‚Äôs embedding `[prompt engineering]` `[tweet]`
* [Advanced Prompt Engineering](https://twitter.com/cwolferesearch/status/1655688722904432640?s=20): cover three important topics in prompt engineering: chain of thought prompting, integration with external databases, and automatic prompt engineering approaches. `[prompt engineering]` `[tweet]`
* [Prompt Engineering for ChatGPT: A Quick Guide to Techniques, Tips, and Best Practices](https://www.linkedin.com/posts/sabitekin_prompt-engineering-for-chatgpt-ugcPost-7060115575208873984-30kS?utm_source=share&utm_medium=member_desktop): Let Genie (ChatGPT) teach you how to make wise Wishes (Prompts) `[prompt engineering]` `[paper]`
* [Prompt Engineering Guide](https://www.promptingguide.ai/pt): üêô Guides, papers, lecture, notebooks and resources for prompt engineering `[prompt engineering]` `[page]`
* [The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/abs/2104.08691): proposes a mechanism for learning soft prompts through backpropagation. `[prompt engineering]` `[paper]`
* [Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/abs/2101.00190): a lightweight alternative to fine-tuning that prepends a trainable continuous prefix for NLG tasks. `[prompt engineering]` `[paper]`
* [Large Language Models Are Human-Level Prompt Engineers](https://arxiv.org/abs/2211.01910): APE for automatic instruction generation and selection, achieving better task performance than previous language models and comparable performance to human-generated prompts on most tasks. `[prompt engineering]` `[paper]`
* [AutoPrompt: Eliciting Knowledge from Language Models with Automatically Generated Prompts](https://arxiv.org/abs/2010.15980): proposes an approach to automatically create prompts for a diverse set of tasks based on gradient-guided search. `[prompt engineering]` `[paper]`
* [ChatGPT Prompt Engineering for Developers](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/): ‚Ä¢ Learn prompt engineering best practices for application development ‚Ä¢ Discover new ways to use LLMs, including how to build your own custom chatbot ‚Ä¢ Gain hands-on practice writing and iterating on prompts yourself using the OpenAI API `[prompt engineering]` `[video]`
* [Chat with any Github Repo](https://twitter.com/pwang_szn/status/1650801868568772608): 1) Scrapes @github repo ü§ñ 2) üóÇÔ∏è Embeds codebase using @LangChainAI, stores embeddings in @activeloopai 3) Chat with the codebase w/ @streamlit üë®‚Äçüíª `[semantic search e Q&A]` `[tweet]`
* [I built a ChatGPT app that lets you chat with any codebase](https://twitter.com/marktenenholtz/status/1651568107192983553?s=20):  `[semantic search e Q&A]` `[tweet]`
* [Search-in-the-Chain: Towards Accurate, Credible and Traceable Large Language Models for Knowledge-intensive Tasks](https://arxiv.org/abs/2304.14732): a novel framework called Search-in-the-Chain (SearChain) to improve the accuracy, credibility and traceability of LLM-generated content for multi-hop question answering `[semantic search e Q&A]` `[paper]`
* [Sabi√°: Portuguese Large Language Models](https://arxiv.org/abs/2304.07880): In this paper, we add to the growing body of evidence that challenges this practice, demonstrating that monolingual pretraining on the target language significantly improves models already extensively trained on diverse corpora. `[training]` `[paper]`
* ["Google ""We Have No Moat, And Neither Does OpenAI"""](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither): Leaked Internal Google Document Claims Open Source AI Will Outcompete Google and OpenAI `[training]` `[post]`
* [Introducing StarCoder](https://twitter.com/BigCodeProject/status/1654174941976068119?s=20): StarCoder is a 15B LLM for code with 8k context and trained only on permissive data in 80+ programming languages. `[training]` `[tweet]`
* [MosaicML MPT-7B](https://www.mosaicml.com/blog/mpt-7b): MPT-7B is a transformer trained from scratch on 1T tokens of text and code. It is open source, available for commercial use, and matches the quality of LLaMA-7B. `[training]` `[page]`
* [Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes](https://arxiv.org/abs/2305.02301): However, finetuning and distillation require large amounts of training data to achieve comparable performance to LLMs. We introduce Distilling step-by-step, a new mechanism that (a) trains smaller models that outperform LLMs, and (b) achieves so by leveraging less training data needed by finetuning or distillation. `[training]` `[paper]`
* [How to run LLaMA 13B with a 6GB card](https://twitter.com/_akhaliq/status/1657779996247588865):  `[training]` `[tweet]`
* [Formal Algorithms for Transformers](https://arxiv.org/abs/2207.09238): This document aims to be a self-contained, mathematically precise overview of transformer architectures and algorithms (not results). It covers what transformers are, how they are trained, what they are used for, their key architectural components, and a preview of the most prominent models. The reader is assumed to be familiar with basic ML terminology and simpler neural network architectures such as MLPs. `[training]` `[]`
* [CodeT5+: Open Code Large Language Models for Code Understanding and Generation](https://arxiv.org/abs/2305.07922):  `[training]` `[paper]`
* [The Stack](https://huggingface.co/datasets/bigcode/the-stack-dedup): The Stack contains over 6TB of permissively-licensed source code files covering 358 programming languages. `[training]` `[github]`
* [Evaluating Large Language Models Trained on Code](https://arxiv.org/abs/2107.03374): We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8% of the problems, while GPT-3 solves 0% and GPT-J solves 11.4%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics. `[training]` `[paper]`
* [How to train your own LLMs](https://blog.replit.com/llm-training):  `[training]` `[post]`
* [Training language models to follow instructions with human feedback](https://arxiv.org/abs/2203.02155):  `[training]` `[]`
* [HumanEval: Hand-Written Evaluation Set](https://github.com/openai/human-eval):  `[training]` `[]`
